%\documentclass[12pt,PhD,twoside]{muthesis}
%\usepackage{verbatim}
%\usepackage{graphicx}
%\graphicspath{ {img/} }
%\usepackage{hyperref}
%\usepackage{subfig}
%\usepackage{xcolor}
%\usepackage{url} % typeset URL's reasonably
%\usepackage{listings}
%\usepackage{pdfpages}
%\usepackage{tabu}
%\usepackage{longtable}
%\usepackage{multirow, tabularx}
%\usepackage[labelfont=bf]{caption}
%\usepackage{natbib}
%\usepackage{pslatex} % Use Postscript fonts
%\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{bm}
%\usepackage{pseudocode}
%\DeclareMathOperator*{\argmin}{arg\,min}
%\DeclareMathOperator*{\argmax}{arg\,max}
%\def\approxprop{%
%	\def\p{%
%		\setbox0=\vbox{\hbox{$\propto$}}%
%		\ht0=0.6ex \box0 }%
%	\def\s{%
%		\vbox{\hbox{$\sim$}}%
%	}%
%	\mathrel{\raisebox{0.7ex}{%
%			\mbox{$\underset{\s}{\p}$}%
%		}}%
%	}
%
%\newcolumntype{?}{!{\vrule width 2pt}}
%
%
%\begin{document}
%\bibliographystyle{model5-names}

\chapter{Metric learning in pairwise difference space}
\label{apx:adsmetric}

\section*{Introduction}

Constraints on which data points should or should not be grouped together (must-link or cannot-link constraints) can significantly improve the performance of clustering. Most conventional semi-supervised approaches try to minimize must-link distances, maximize cannot-link distances, or to enforce a constant limit on either and optimize the other. They do not make explicit use of the probability distribution of must-link and cannot-link distances. 

Here, we propose a framework for semi-supervised clustering based on projecting the data into a distance space in which distances reflect the linkage probabilities of belonging to the same cluster, using simple probabilistic classifiers trained on the available constraints. The framework can be seen as a novel approach to perform non-linear metric learning using weak supervision in the form of pairwise constraints, in order to improve clustering performance, as pioneered by \citep{xing2002distance}. Although this problem is very similar to metric learning in general, the criterion of interest is often somewhat different: as opposed to optimizing the performance of some classifier as in e.g. \citep{bellet2012similarity}), or for a large margin as in e.g. \citep{weinberger2005distance}, the goal is ensuring that all instances of a cluster are closer under the learned metric than those of different clusters. Furthermore, semi-supervised methods requiring partially labelled instances are not applicable if only pairwise constraints are available. 

We have used this framework to model the structure of spatial representations in human participants in Chapter 5 above, using the information which buildings have or have not been co-represented as the must-link and cannot-link constraints to train a Gaussian Discriminant Analysis (GDA) model in absolute pairwise difference space (see also Chapter 2.5 for the mathematical formulation, and Chapter 5 for the results). 

\section*{Motivation}

For general applicability, non-linear metrics are vital, because of the problem of \textit{multimodality}. When data points forming several groups or `modes' in unweighted feature space actually belong to the same cluster semantically, as indicated by ML constraints, there exists no linear projection able to separate them, and linear methods must inevitably fail. A traditional example is the XOR dataset, consisting of four groups, connected by diagonal ML constraints, such that there exists no linear separating hyperplane - see Figure \ref{fig:motivation2}a. 

Although kernel-based methods can deal with multimodal clustering problems (or any complicated data distribution) according to the Representer Theorem, in theory, given the optimal kernel and suitable parameters, in practice it is often difficult to find such a kernel. Most non-linear metric learning methods able to learn suitable kernels are sensitive to multiple hyperparameters, and, being nonconvex optimization problems, frequently get stuck in local minima.

A further issue with popular isotropic kernels such as the Radial Basis Function (RBF), frequently used in non-linear metric learning \citep{baghshah2010kernel, chitta2011approximate}, is that they are ill-suited for data with features on very different scales, since the optimal regularization parameters in one dimension can be suboptimal in other dimensions in this case, as pointed out by \citep{ong2005learning} (who propose a solution only in the supervised setting). 

\begin{figure*}[t]
	\centering
	
	\subfloat[XOR dataset]{%
		\includegraphics[width=0.25\textwidth]{quadplot1}%
	}
	\subfloat[Pairwise differences]{%
		\includegraphics[width=0.25\textwidth]{quadplot2}%
	}
	\subfloat[Non-isotropic variance]{%
		\includegraphics[width=0.25\textwidth]{isosvmproblem1}%
	}
	\subfloat[Pairwise differences]{%
		\includegraphics[width=0.25\textwidth]{isosvmproblem2}%
	}
	
	\caption[Motivation for the proposed metric learning approach]{Motivation for the proposed metric learning approach. (a) Example not linearly separable data requiring non-linear metrics. (b) Visualization of the distribution of corresponding absolute pairwise differences (APD), containing the element-wise differences in all dimensions between all possible objects, within (black) and across (coloured) clusters. The background contour shows the probability of a pair with a given distance belonging to the same cluster, learned by Gaussian Discriminant Analysis, and used as the distance pseudometric. Red triangles show the labelled ML and CL constraints. (c) Example data with non-isotropic variance (three orders of magnitude larger in the y-axis direction). (d) Corresponding APD space coloured as in (a). In addition to the modelled probability of belonging to the same cluster, the models decision boundary (black), as well as the decision boundary of a Support Vector Machine with an isotropic RBF kernel (optimal parameters set by grid search), which overfits along the low-variance dimension.}
	\label{fig:motivation2}
\end{figure*}
 
Our motivations for proposing a novel non-linear method are threefold. First, we would like to sidestep the difficulty of robustly finding a good non-linear metric for a particular dataset, in a probabilistic framework, without hyperparameter tuning. Second, instead of learning a metric using an objective function based on Lp-distance, which collapses the differences along the individual dimensions into one value, we would like to let the model directly access these individual differences, and thus to learn their importance, allowing it to easily deal with non-isotropic data (Figure \ref{fig:motivation2}c-d). Third, we would like to make use of prior information regarding what constitutes a `good' metric for clustering. In particular, unlike using a weighted Lp-metric which attempts to summarize the structure of within-cluster and across-cluster pairwise distances as scalars, we define \textit{a pseudo-metric based on the vector space of absolute pairwise differences} (APD). 

This not only allows straightforward learning of the importance of each feature and thus fitting non-isotropic distributions better than isotropic kernels (Figure \ref{fig:motivation2}c-d), but also makes explicit the structure in the distribution of constraints. It has been observed before that for data containing clusters, the probability density function of pairwise Lp distances shows two peaks (one for within- and one for across-cluster pairs), e.g. by \citep{brin1995near}. However, in the case of multiple clusters with different shapes and variances, a bimodal distribution is insufficient to reflect the true distributions of the instance differences within or across clusters. Clearly, within-cluster variances in one cluster do not have to equal those in another cluster, and the same is true for across-cluster variances (as illustrated by the variances of the groups of data in APD space in Figure \ref{fig:motivation2}b and d). Learning in pairwise difference vector space (instead of collapsing these distances into scalars) allows our model to adapt locally to within- and across-cluster variances of different clusters, and therefore to better approximate the true pairwise distance distribution. 

\section{Supervised learning in absolute pairwise difference space}

The formulation of (weakly) supervised learning in absolute pairwise difference space was given in Chapter 2.5 above. We briefly summarize it as follows.

Let $\mathcal{X}=(\boldsymbol x_i, ..., \boldsymbol x_n)$ be the feature vector representation of $n$ objects which are to be clustered, where $x_i \in \mathbb{R}^D$ are vectors with $D$ dimensions. Let the set of $m$ given pairwise linkage constraints be denoted by $\mathcal{L}$, where $ \lvert \mathcal{L} \lvert = m $, and $l_{i,j} \in \mathcal{L}$ is

\begin{equation}
l_{i,j}=
\begin{cases}
1, & \text{if $i$ and $j$ belong to the same cluster (ML constraint)} \\
0, & \text{if $i$ and $j$ belong to different clusters (CL constraint)}
\end{cases}
\end{equation}

Then, a distance metric $d_l$ between two instances can be defined based on the probability that these instances belong to the same cluster, making use of a generative classifier (constituent model) trained on the given constraints:

\begin{equation}
\label{eq:metric2}
d_l(\boldsymbol x_1, \boldsymbol x_2; \boldsymbol{\theta}) = 1 - p(l=1|\Delta \boldsymbol x, \boldsymbol{\theta}) = p(l=0|\Delta \boldsymbol x, \boldsymbol{\theta})
\end{equation}

There are three advantages of using generative classifiers instead of just the classification output as the pseudometric. First, this makes the model well-suited to learning in the inductive setting (where the examples from the test set are not seen at training time). Many existing approaches are designed for and evaluated in the transductive setting. Second, on more complex, unseparable data, utilizing the models confidence of whether instances should be linked, in addition to a binary prediction, greatly improves the resulting clustering. This choice also allows choosing suitable priors and likelihoods and thus tailoring the model to fit the data at hand.

The learned metric in \eqref{eq:metric2} can subsequently be embedded into a new feature space using multi-dimensional scaling (MDS), and then used for clustering or classification.

\section{Extension of the framework to semi-supervised learning}

Given the large number of pairwise differences, ${n \choose 2}=\frac{n^2-n}{2}$, the following question arises: why only use the tiny set of given pairwise constraints for learning, instead of making use of the entire distribution in APD space?

An extension of a recently proposed framework for semi-supervised learning, called Contrastive Pessimistic Likelihood Estimation (CPLE) \citep{loog2015contrastive}, can facilitate such an approach for using the entire APD space. \cite{loog2015contrastive} points out that most current semi-supervised approaches are not `safe' (do not guarantee better performance when including the unlabelled data than without), and often perform sub-optimally due to model misspecification (making assumptions violated by data). Instead, he suggests only using the intrinsic assumptions of a given classifier, and training it in a pessimistic framework: using optimization to find hypothetical labels for the unlabelled data corresponding to the worst case. This pessimism ensures that the inclusion of unlabelled data point cannot make the model accuracy worse than just using the labelled data. 

In practice, his procedure pessimistically assign soft labels to the unlabelled data, such that the improvement over the supervised version is minimal; at the same time maximize log likelihood over labelled data. That is, the parameters $\theta_{semi}$ of a semi-supervised model in the CPLE framework are given by

\begin{equation}
\label{eq:cple}
\theta_{semi} = \argmax_\theta \argmin_q L(\theta|X,U,q) - L(\theta_{sup}|X,U,q),
\end{equation}

where $X={(x_i,y_i)}_{i=1}^N$ denotes the labelled data, $U$ the unlabelled data, and $q$ the hypothetical soft labels. \citep{loog2015contrastive} only provides a solution for Linear Discriminant Analysis (LDA), and his framework requires an explicit generative likelihood $L(\theta|X,U,q)$.

However, his framework can be modified to use discriminative likelihoods instead. This allows using both generative and discriminative classifiers in this framework, provided that they can output a prediction probability (such probability estimates can also be provided by Platt scaling \citep{platt1999probabilistic} for classifiers which don't support them). 

Figure \ref{cplealg} shows the objective function this extended CPLE model, supporting pessimistic semi-supervised learning with any constituent model. Instead of optimizing the generative likelihood $L$, the negative log loss $J(y,r)=log p(y|r)=\frac{1}{N} \sum_{i=1}^N (y_i \text{log} (r_i) + (1-y_i) \text{log} (1-r_i))$ is used in the optimization objective, where $y_i$ is the predicted class of the $i$'th instance, and $r_i$ is the probability associated with this prediction.

\begin{figure}[h]
	\begin{pseudocode}{cpleObjectiveFunction}{model, \theta, q, X, y, U}
		1: \text{set} \enspace z_i \GETS \begin{cases}
			1 & \text{if } q_i \geq 0.5\\
			0 & \text{otherwise}
		\end{cases} \enspace \text{for all} \enspace i \in [1, |U|] \\
		2: X' \GETS X \cup U \\
		3: y' \GETS y \cup z \\
		4: \text{set} \enspace w_i \GETS \begin{cases}
			1 & \text{if } X'_i \in X \\
			q_i & \text{otherwise}
		\end{cases} \enspace \text{for all} \enspace i \in [1, |X'|] \\
		5: model \GETS train(model, X', y', w) \\
		6: r \GETS predictionprobability(model, X) \\
		7: r' \GETS predictionprobability(model, U) \\
		8: J_{labelled} \GETS \frac{1}{|X|} \sum_{i=1}^{|X|} (y_i \text{log} (r_i) + (1-y_i) \text{log} (1-r_i)) \\
		9: J_{unlabelled} \GETS \frac{1}{|U|} \sum_{i=1}^{|U|} (z_i \text{log} (r'_i) + (1-z_i) \text{log} (1-r'_i)) \\
		10: return(J_{unlabelled} - J_{labelled})
	\end{pseudocode}
	\caption[Objective function for a general pessimistic semi-supervised learning framework]{\textbf{A general pessimistic semi-supervised learning framework,} based on a generalization of \citep{loog2015contrastive} to use discriminative likelihoods and to be usable with any classifier supporting prediction probabilities. It requires a constituent model, current hypothetical labels $q$ of the unlabelled data points and model parameters $\theta$ which are to be optimized, the unlabelled data $U$, labelled data $X$, labels $y$ of $X$, and the negative log loss $J_{labelled}$ of the model trained only on $X$. Minimizing this objective increases accuracy over the labelled data while assuming worst-case labels for the unlabelled data.}
	\label{cplealg}
\end{figure}


The modified CPLE objective function in Figure \ref{cplealg} can be used with any global optimization approach to train a model to fit the unlabelled data distribution in a pessimistic fashion, while maximizing performance over the labelled data. We used the locally biased variant \citep{gablonsky2001locally} of DIRECT (DIviding RECTangles) \citep{jones1993lipschitzian}, a global, deterministic, derivative-free optimization method based on Lipschitzian optimization, which can handle the kinds of non-linear and non-convex functions constituted by Figure \ref{cplealg}. 

The resulting semi-supervised learning framework is highly computationally expensive, but has the advantages of being a generally applicable framework, needing low memory, and making no additional assumptions except for the ones made by the chosen classifier. It has been made freely available online at \url{https://github.com/tmadl/semisup-learn}, together with a more detailed description.

\section{Constituent models}

%TODO derive minimum no. of constraints

In principle, model able to produce probability estimates could be used as the constituent model in \eqref{eq:metric2} to model the linkage probability distribution. Here, we focus on three models of this class: Gaussian Discriminant Analysis (GDA), Gaussian Process (GP), and Random Forest (RF). The first (GDA) has a closed form solution, thus constituting the - to our knowledge - first probabilistically motivated approach to learning a non-linear distance metric in closed form.
%Thus, they facilitate the - to our knowledge - first probabilistically motivated approach to learning a non-linear distance metric in closed form. \footnote{GPs have a closed-form solution (the best linear unbiased prediction) if the autocorrelation model is known; otherwise, it can be estimated using Expectation Maximization \citep{gpml06}}. 

%TODO rewrite all this shit. use gaussian mixture model discriminant analysis. possibly use multinomial mixture model - p. 5 and 6 in http://math.univ-lille1.fr/~biernack/index_files/articleJSS.pdf
%http://www.mixmod.org/IMG/pdf/statdoc_2014.pdf
%http://www.r-bloggers.com/a-brief-look-at-mixture-discriminant-analysis/

In the case of the \textbf{GDA} \citep{bensmail1996regularized}, the likelihoods of a pair of instances belonging to the same cluster $p( \Delta \textbf x | l=1; \mu_1, \Sigma_1)$ or to different clusters $p( \Delta \textbf x | l=0; \mu_0, \Sigma_0) $, are modelled using multivariate Gaussians:

\begin{equation}
\label{eq:gda}
p( \Delta \textbf x | l=i; \mu_i, \Sigma_i) = (2\pi)^{-\frac{D}{2}}|\boldsymbol\Sigma_i|^{-\frac{1}{2}}\, e^{ -\frac{1}{2}(\Delta \mathbf{x}-\boldsymbol\mu_i)^\intercal\boldsymbol\Sigma_i^{-1}(\Delta \mathbf{x}-\boldsymbol\mu_i) },
\end{equation}

where $i \in \{0,1\}$. $(\mu_1, \Sigma_1)$ are the means and covariances of the APD distances of pairs in the same cluster, and $(\mu_0, \Sigma_0)$ those in different clusters. These parameters can be easily estimated from the instances corresponding to the must-link and cannot-link constraints, respectively, by calculating their means and covariances. 

%In the case of \textbf{GDA} \citep{bensmail1996regularized}, the likelihood of a pair of instances belonging to one of the $K+A$ assumed APD distributions $\mathcal{D}_i$ (which can be either a within-cluster or across-cluster distribution),  are modelled using multivariate Gaussians: 

%\begin{equation}
%\label{eq:mvnormal}
%p( \Delta \textbf x | \Delta \textbf x \in \mathcal{D}_i; \mu_i, \Sigma_i) = (2\pi)^{-\frac{D}{2}}|\boldsymbol\Sigma_i|^{-\frac{1}{2}}\, e^{ -\frac{1}{2}(\Delta \mathbf{x}-\boldsymbol\mu_i)^\intercal\boldsymbol\Sigma_i^{-1}(\Delta \mathbf{x}-\boldsymbol\mu_i) }
%\end{equation}

%, and estimated from the given sets of must-link and cannot-link constraints $\mathcal{L}^{ML}_k \in \mathcal{L}$ and $\mathcal{L}^{CL}_k \in \mathcal{L}$

%$(\mu_i, \Sigma_i)$ are the means and covariances of the APD distances of $\Delta \boldsymbol{x}_j \in \mathcal{L}^{ML}_k$ for $\mathcal{D}=\mathcal{W}$, and of $\Delta \boldsymbol{x}_j \in \mathcal{L}^{CL}_k$ for $\mathcal{D}=\mathcal{A}$, and can be calculated analytically. The priors $p(\Delta \textbf x \in \mathcal{D}_i)=\frac{1}{m} |\mathcal{L}_i|$ simply reflect the relative frequency of the constraints in the relevant subset.

In the case of \textbf{GP}, we model the linkage probability distribution using a Gaussian Process with mean function $m(\cdot)$ and covariance function $k(\cdot)$:  

% linear kernel - actually doing Bayesian linear regression, can be done in O(n)! Try scikits' BayesianRidge and/or Ridge Classifier http://people.seas.harvard.edu/~dduvenaud/cookbook/

\begin{equation}
\label{eq:gp}
p(\Delta \textbf x | l=i; M, K) = \mathcal{GP}(m(\cdot), k(\cdot)) = \mathcal{N}\Bigg( \begin{bmatrix}m(x_1) \\ \vdots \\ m(x_m) \end{bmatrix},  \begin{bmatrix}k(x_1, x_1), ..., k(x_1, x_m) \\ \vdots \quad \quad \ddots \quad \quad \vdots \\ k(x_m, x_1), ..., k(x_m, x_m)\end{bmatrix}  \Bigg)
\end{equation}

In the experiments below, we have used a zero mean function and the square exponential kernel as the covariance function. See \citep{rasmussen2005gp} for an extensive book on Gaussian Processes.

Finally, in the case of \textbf{RF}, an ensemble of $B$ decision trees is learned. Each decision tree is trained on a bootstrap resample of data (drawn with replacement), and only considering a randomly selected subset of the available features. See \citep{breiman2001random} for details. There are several ways to obtain class probabilities from trained random forests \citep{bostrom2007estimating}. We used the relative class frequency, i.e. the averaged fractions of samples falling on that same class in the leaves of individual trees:

\begin{equation}
\label{eq:rf}
p(\Delta \textbf x | l=i; M, K) = \frac{1}{B} \sum_{j=1}^{B} \frac{l(t_j, \Delta \textbf x, i)}{l(t_j, \Delta \textbf x, i)+l(t_j, \Delta \textbf x, 1-i)},
\end{equation}

where $t_j$ denotes the individual trained trees, and $l(t_j, \Delta \textbf x, i)$ is a function returning the number of training examples falling into the same leaf as $\Delta \textbf x$ in tree $t_j$.

%
%For Multinomial \textbf{Naive Bayes} \citep{kibriya2005multinomial}, which is suitable for e.g. text data, the likelihood given $K$ event counts (e.g. word counts) is simply
%
%\begin{equation}
%\label{eq:nb}
%p(\Delta \textbf x | l=i; \Phi_{0}, \Phi_{1}) = \frac{(\sum_j \Delta x_j)!}{\prod_j \Delta x_j!} \prod_j p_{ij}^{\Delta x_j},
%\end{equation}
%
%where $p_{ij}$ are the probabilities that word $j$ occurs within ($i=1$) or across ($i=0$) a cluster of documents, estimated from the provided constraints. The feature vectors $x$, instead of just traditional word frequencies, are calculated using term frequency and inverse document frequency for each word, as suggested by \citep{rennie2003tackling}.

\section{Preliminary results}

We will first assume a purely supervised case - plugging in a GDA model (Equation \eqref{eq:gda}) into the APD metric framework (Equation \eqref{eq:metric2}), as used to model human spatial representation structure in Chapters 2.5 and 5. We have claimed in Chapter 2.5 that our metric is well-suited for clustering, in the sense that for most within-cluster differences $\Delta \boldsymbol x_r$ and across-cluster differences $\Delta \boldsymbol x_n $ it holds that $ d_r(\boldsymbol x_{r,1}, \textbf x_{r,2}; \boldsymbol{\theta}) < d_n(\boldsymbol x_{n,1}, \textbf x_{n,2}; \boldsymbol{\theta}) $. 

Figure \ref{embedding} shows distances under our metric in three example datasets, embedded into two dimensions for visualization using MDS. It contrasts these distances with embeddings by t-SNE \citep{van2008visualizing}, a state of the art method for visualization, designed to represent high-dimensional data points in such a way that similar objects are modelled by nearby points, and dissimilar objects by distant points. Figure \ref{embedding} shows that the APD metric `pulls together' different clusters (shown using different colours) more effectively than t-SNE. 

\begin{figure*}[t]
	\centering
	
	\includegraphics[width=\textwidth]{embeddingresults}%
	
	\caption[Embedding of pairwise distances as suggested by our metric learning approach]{Embedding of pairwise distances as suggested by our metric learning approach (top) and by t-SNE (bottom) on the COIL-3 dataset (128x128px images of 3 similar-looking toy cars) used by \citep{zeng2012semi}, the ionosphere and wheat datasets from the UCI machine learning repository, and the lymphoma microarray dataset obtained from the Broad Institute. The APD plot was created using supervised GDA as the constituent model.}
	\label{embedding} 
\end{figure*}

The Figure is intended to show that datasets with no clear intrinsic cluster structure can be made separable by learning in APD space given only a few pairwise constraints (20 in these examples, which is less than $0.1 \%$ of all pairwise constraints), even with a fully supervised constituent model. It is not intended as a fair comparison, as t-SNE is an unsupervised method.

\subsection*{Clustering results on benchmark datasets}

We have compared the described metric learning framework in APD space against several state of the art semi-supervised clustering models on multiple datasets. The constituent models in this section were trained in a semi-supervised fashion. In the case of GDA, we applied the extended CPLE learning principle shown in Figure \ref{cplealg} in APD space. In the case of the LS (label spreading) constituent model, we applied the learning algorithm by \citep{zhou2004learning}, which can be described as a spreading activation model, labelling unlabelled data points based on their closest neighbours. 

Tables \ref{tblmany} and \ref{tblfew} summarize the performance of semi-supervised clustering with APD metrics using these constituent models, compared to other recent approaches. In all cases, our model learned the metric in APD space, inferred and embedded all pairwise distances based on this metric, and then performed K-Means clustering.


\begin{table*}[h]
\begin{tabularx}{\textwidth}{|c|c|c|c|c|c?c|c|}
	\cline{1-8}
	dataset & \begin{tabular}[c]{@{}c@{}} \text{cons-}\\\text{traints} \end{tabular} & \begin{tabular}[c]{@{}c@{}} \text{DCA+}\\\text{KMeans} \end{tabular} & \text{Kmeans} & \begin{tabular}[c]{@{}c@{}} \text{MPC}\\\text{KMeans} \end{tabular} & \begin{tabular}[c]{@{}c@{}} \text{SS-}\\\text{MMC} \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{APD-}\\\textbf{GDA} \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{APD-}\\\textbf{LS} \end{tabular} \\ \cline{1-8}
	\multirow{4}{*}{pendigit} & 40 & 0.328 & 0.545 & 0.556 & 0.743 & 0.554 & \textcolor{blue}{\textbf{0.874}} \\
	& 60 & 0.553 & 0.545 & 0.535 & 0.831 & 0.717 & \textcolor{blue}{\textbf{0.855}} \\ 
	& 80 & 0.677 & 0.545 & 0.58 & 0.833 & 0.769 & \textcolor{blue}{\textbf{0.901}} \\ 
	& 100 & 0.721 & 0.545 & 0.537 & 0.861 & 0.8 & \textcolor{blue}{\textbf{0.878}} \\ \cline{1-8}
	\multirow{4}{*}{letterIJL} & 40 & 0.207 & 0.266 & 0.223 & \textbf{0.546} & 0.194 & 0.467 \\ 
	& 60 & 0.352 & 0.266 & 0.226 & \textbf{0.691} & 0.3 & 0.431 \\ 
	& 80 & 0.4 & 0.266 & 0.167 & \textbf{0.568} & 0.417 & 0.329 \\ 
	& 100 & 0.582 & 0.266 & 0.211 & \textbf{0.655} & 0.517 & 0.571 \\ \cline{1-8}
	\multirow{4}{*}{vehicle} & 100 & 0.233 & 0.186 & 0.122 & \textbf{0.277} & 0.161 & 0.208 \\ 
	& 150 & 0.331 & 0.186 & 0.122 & \textbf{0.365} & 0.248 & 0.215 \\ 
	& 200 & 0.418 & 0.186 & 0.102 & \textbf{0.411} & 0.355 & 0.288 \\ 
	& 250 & 0.459 & 0.186 & 0.113 & \textbf{0.458} & 0.351 & 0.26 \\ \cline{1-8}
	\multirow{4}{*}{ionosphere} & 40 & 0.205 & 0.135 & 0.101 & \textbf{0.242} & 0.181 & 0.174 \\ 
	& 60 & 0.167 & 0.135 & 0.094 & \textbf{0.304} & 0.293 & 0.148 \\ 
	& 80 & 0.197 & 0.135 & 0.078 & 0.297 & \textcolor{blue}{\textbf{0.332}} & 0.216 \\ 
	& 100 & 0.199 & 0.135 & 0.094 & 0.313 & \textcolor{blue}{\textbf{0.419}} & 0.282 \\ \cline{1-8}
\end{tabularx}
\caption[Semi-supervised clustering comparison - many constraints]{\textbf{Semi-supervised clustering comparison - many constraints ($10^1-10^3$).} The table shows results of the APD-space metric learning framework applied to clustering, with two constituent models, one using GDA and one using label spreading (LS) \citep{zhou2004learning}. Compared methods: Discriminative Component Analysis (DCA)+KMeans \citep{hoi2006learning}, KMeans, MPCKmeans \citep{bilenko2004integrating}, and Semi-Supervised Maximum Margin Clustering \citep{zeng2012semi}. Non-APD model performances were taken from \citep{zeng2012semi}.}
\label{tblmany}
\end{table*}

\begin{table*}[h]
	\includegraphics[width=\textwidth]{veryfewconstraintstbl}
	
	\caption[Semi-supervised clustering comparison - few constraints]{\textbf{Semi-supervised clustering comparison - few constraints ($10^0-10^1$).} The table shows results of the APD-space metric learning framework applied to clustering, with two constituent models, one using GDA and one using label spreading (LS) \citep{zhou2004learning}. Compared methods: MPC-Kmeans \citep{bilenko2004integrating}, Constraint Projection (CP)-Kmeans \citep{tang2007enhancing}, Constraint Neighbourhood Projection (CNP)-Kmeans \citep{wang2014constraint}, Principal Component Analysis (PCA)-Kmeans, GP-Kmeans \citep{eaton2005clustering} and linear discriminant analysis (LDA)-Kmeans \citep{ding2007adaptive}. Non-APD model performances were taken from \citep{wang2014constraint}. `OOM' stands for a model running out of memory.}
	\label{tblfew}
\end{table*}

%\clearpage

\subsection*{Clustering results on cancer microarray data}

Finally, we tested the other described constituent models on a bioinformatics dataset, to show its utility on non-benchmark problems with real-world relevance. It was recently proposed that semi-supervised consensus clustering (SSCC) outperforms the state of the art on cancer microarray data \citep{wang2014semi}. We compared our APD-space based clustering with this ensemble clustering method, using the GP and RF constituent models. 

\begin{figure*}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{leukemiaresults}
	
	\caption[Semi-supervised clustering results on cancer microarray data]{\textbf{Semi-supervised clustering results on cancer microarray data} from \citep{golub1999molecular}, compared to the state of the art semi-supervised consensus clustering method by \citep{wang2014semi}.}
	\label{leukemia}
\end{figure*}

Figure \ref{leukemia} shows the results on the Leukemia dataset, published and made available by \citep{golub1999molecular}. Despite the SSCC being an ensemble method, the APD-space methods significantly outperform it when given a sufficient number of constraints.

%\renewcommand\bibname{References for Appendix E}
%
%\begin{thebibliography}{26}
%	\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
%	\providecommand{\url}[1]{\texttt{#1}}
%	\providecommand{\href}[2]{#2}
%	\providecommand{\path}[1]{#1}
%	\providecommand{\DOIprefix}{doi:}
%	\providecommand{\ArXivprefix}{arXiv:}
%	\providecommand{\URLprefix}{URL: }
%	\providecommand{\Pubmedprefix}{pmid:}
%	\providecommand{\doi}[1]{\href{http://dx.doi.org/#1}{\path{#1}}}
%	\providecommand{\Pubmed}[1]{\href{pmid:#1}{\path{#1}}}
%	\providecommand{\bibinfo}[2]{#2}
%	\ifx\xfnm\relax \def\xfnm[#1]{\unskip,\space#1}\fi
%	%Type = Article
%	\bibitem[{Baghshah \& Shouraki(2010)}]{baghshah2010kernel}
%	\bibinfo{author}{Baghshah, M.~S.}, \& \bibinfo{author}{Shouraki, S.~B.}
%	(\bibinfo{year}{2010}).
%	\newblock \bibinfo{title}{Kernel-based metric learning for semi-supervised
%		clustering}.
%	\newblock {\it \bibinfo{journal}{Neurocomputing}\/},  {\it
%		\bibinfo{volume}{73}\/}, \bibinfo{pages}{1352--1361}.
%	%Type = Article
%	\bibitem[{Bensmail \& Celeux(1996)}]{bensmail1996regularized}
%	\bibinfo{author}{Bensmail, H.}, \& \bibinfo{author}{Celeux, G.}
%	(\bibinfo{year}{1996}).
%	\newblock \bibinfo{title}{Regularized gaussian discriminant analysis through
%		eigenvalue decomposition}.
%	\newblock {\it \bibinfo{journal}{Journal of the American statistical
%			Association}\/},  {\it \bibinfo{volume}{91}\/}, \bibinfo{pages}{1743--1748}.
%	%Type = Inproceedings
%	\bibitem[{Bilenko et~al.(2004)Bilenko, Basu \& Mooney}]{bilenko2004integrating}
%	\bibinfo{author}{Bilenko, M.}, \bibinfo{author}{Basu, S.}, \&
%	\bibinfo{author}{Mooney, R.~J.} (\bibinfo{year}{2004}).
%	\newblock \bibinfo{title}{Integrating constraints and metric learning in
%		semi-supervised clustering}.
%	\newblock In {\it \bibinfo{booktitle}{Proceedings of the twenty-first
%			international conference on Machine learning}\/} (p.~\bibinfo{pages}{11}).
%	\newblock \bibinfo{organization}{ACM}.
%	%Type = Inproceedings
%	\bibitem[{Bostr{\"o}m(2007)}]{bostrom2007estimating}
%	\bibinfo{author}{Bostr{\"o}m, H.} (\bibinfo{year}{2007}).
%	\newblock \bibinfo{title}{Estimating class probabilities in random forests}.
%	\newblock In {\it \bibinfo{booktitle}{Machine Learning and Applications, 2007.
%			ICMLA 2007. Sixth International Conference on}\/} (pp.
%	\bibinfo{pages}{211--216}).
%	\newblock \bibinfo{organization}{IEEE}.
%	%Type = Article
%	\bibitem[{Breiman(2001)}]{breiman2001random}
%	\bibinfo{author}{Breiman, L.} (\bibinfo{year}{2001}).
%	\newblock \bibinfo{title}{Random forests}.
%	\newblock {\it \bibinfo{journal}{Machine learning}\/},  {\it
%		\bibinfo{volume}{45}\/}, \bibinfo{pages}{5--32}.
%	%Type = Inproceedings
%	\bibitem[{Brin(1995)}]{brin1995near}
%	\bibinfo{author}{Brin, S.} (\bibinfo{year}{1995}).
%	\newblock \bibinfo{title}{Near neighbor search in large metric spaces}.
%	\newblock In \bibinfo{editor}{U.~Dayal}, \bibinfo{editor}{P.~M.~D. Gray}, \&
%	\bibinfo{editor}{S.~Nishio} (Eds.), {\it \bibinfo{booktitle}{{VLDB} '95:
%			proceedings of the 21st International Conference on Very Large Data Bases,
%			Zurich, Switzerland, Sept. 11--15, 1995}\/} (pp. \bibinfo{pages}{574--584}).
%	\newblock \bibinfo{address}{Los Altos, CA 94022, USA}:
%	\bibinfo{publisher}{Morgan Kaufmann Publishers}.
%	%Type = Inproceedings
%	\bibitem[{Chitta et~al.(2011)Chitta, Jin, Havens \&
%		Jain}]{chitta2011approximate}
%	\bibinfo{author}{Chitta, R.}, \bibinfo{author}{Jin, R.},
%	\bibinfo{author}{Havens, T.~C.}, \& \bibinfo{author}{Jain, A.~K.}
%	(\bibinfo{year}{2011}).
%	\newblock \bibinfo{title}{Approximate kernel k-means: Solution to large scale
%		kernel clustering}.
%	\newblock In {\it \bibinfo{booktitle}{Proceedings of the 17th ACM SIGKDD
%			international conference on Knowledge discovery and data mining}\/} (pp.
%	\bibinfo{pages}{895--903}).
%	\newblock \bibinfo{organization}{ACM}.
%	%Type = Inproceedings
%	\bibitem[{Ding \& Li(2007)}]{ding2007adaptive}
%	\bibinfo{author}{Ding, C.}, \& \bibinfo{author}{Li, T.} (\bibinfo{year}{2007}).
%	\newblock \bibinfo{title}{Adaptive dimension reduction using discriminant
%		analysis and k-means clustering}.
%	\newblock In {\it \bibinfo{booktitle}{Proceedings of the 24th international
%			conference on Machine learning}\/} (pp. \bibinfo{pages}{521--528}).
%	\newblock \bibinfo{organization}{ACM}.
%	%Type = Phdthesis
%	\bibitem[{Eaton(2005)}]{eaton2005clustering}
%	\bibinfo{author}{Eaton, E.~R.} (\bibinfo{year}{2005}).
%	\newblock {\it \bibinfo{title}{Clustering with Propagated Constraints}\/}.
%	\newblock Ph.D. thesis Citeseer.
%	%Type = Article
%	\bibitem[{Gablonsky \& Kelley(2001)}]{gablonsky2001locally}
%	\bibinfo{author}{Gablonsky, J.~M.}, \& \bibinfo{author}{Kelley, C.~T.}
%	(\bibinfo{year}{2001}).
%	\newblock \bibinfo{title}{A locally-biased form of the direct algorithm}.
%	\newblock {\it \bibinfo{journal}{Journal of Global Optimization}\/},  {\it
%		\bibinfo{volume}{21}\/}, \bibinfo{pages}{27--37}.
%	%Type = Article
%	\bibitem[{Golub et~al.(1999)Golub, Slonim, Tamayo, Huard, Gaasenbeek, Mesirov,
%		Coller, Loh, Downing, Caligiuri et~al.}]{golub1999molecular}
%	\bibinfo{author}{Golub, T.~R.}, \bibinfo{author}{Slonim, D.~K.},
%	\bibinfo{author}{Tamayo, P.}, \bibinfo{author}{Huard, C.},
%	\bibinfo{author}{Gaasenbeek, M.}, \bibinfo{author}{Mesirov, J.~P.},
%	\bibinfo{author}{Coller, H.}, \bibinfo{author}{Loh, M.~L.},
%	\bibinfo{author}{Downing, J.~R.}, \bibinfo{author}{Caligiuri, M.~A.} et~al.
%	(\bibinfo{year}{1999}).
%	\newblock \bibinfo{title}{Molecular classification of cancer: class discovery
%		and class prediction by gene expression monitoring}.
%	\newblock {\it \bibinfo{journal}{science}\/},  {\it \bibinfo{volume}{286}\/},
%	\bibinfo{pages}{531--537}.
%	%Type = Inproceedings
%	\bibitem[{Hoi et~al.(2006)Hoi, Liu, Lyu \& Ma}]{hoi2006learning}
%	\bibinfo{author}{Hoi, S.~C.}, \bibinfo{author}{Liu, W.}, \bibinfo{author}{Lyu,
%		M.~R.}, \& \bibinfo{author}{Ma, W.-Y.} (\bibinfo{year}{2006}).
%	\newblock \bibinfo{title}{Learning distance metrics with contextual constraints
%		for image retrieval}.
%	\newblock In {\it \bibinfo{booktitle}{Computer Vision and Pattern Recognition,
%			2006 IEEE Computer Society Conference on}\/} (pp.
%	\bibinfo{pages}{2072--2078}).
%	\newblock \bibinfo{organization}{IEEE} volume~\bibinfo{volume}{2}.
%	%Type = Article
%	\bibitem[{Jones et~al.(1993)Jones, Perttunen \&
%		Stuckman}]{jones1993lipschitzian}
%	\bibinfo{author}{Jones, D.~R.}, \bibinfo{author}{Perttunen, C.~D.}, \&
%	\bibinfo{author}{Stuckman, B.~E.} (\bibinfo{year}{1993}).
%	\newblock \bibinfo{title}{Lipschitzian optimization without the lipschitz
%		constant}.
%	\newblock {\it \bibinfo{journal}{Journal of Optimization Theory and
%			Applications}\/},  {\it \bibinfo{volume}{79}\/}, \bibinfo{pages}{157--181}.
%	%Type = Article
%	\bibitem[{Loog(2015)}]{loog2015contrastive}
%	\bibinfo{author}{Loog, M.} (\bibinfo{year}{2015}).
%	\newblock \bibinfo{title}{Contrastive pessimistic likelihood estimation for
%		semi-supervised classification}.
%	\newblock {\it \bibinfo{journal}{arXiv preprint arXiv:1503.00269}\/}, .
%	%Type = Article
%	\bibitem[{Van~der Maaten \& Hinton(2008)}]{van2008visualizing}
%	\bibinfo{author}{Van~der Maaten, L.}, \& \bibinfo{author}{Hinton, G.}
%	(\bibinfo{year}{2008}).
%	\newblock \bibinfo{title}{Visualizing data using t-sne}.
%	\newblock {\it \bibinfo{journal}{Journal of Machine Learning Research}\/},
%	{\it \bibinfo{volume}{9}\/}, \bibinfo{pages}{85}.
%	%Type = Inproceedings
%	\bibitem[{Ong et~al.(2005)Ong, Williamson \& Smola}]{ong2005learning}
%	\bibinfo{author}{Ong, C.~S.}, \bibinfo{author}{Williamson, R.~C.}, \&
%	\bibinfo{author}{Smola, A.~J.} (\bibinfo{year}{2005}).
%	\newblock \bibinfo{title}{Learning the kernel with hyperkernels}.
%	\newblock In {\it \bibinfo{booktitle}{Journal of Machine Learning Research}\/}
%	(pp. \bibinfo{pages}{1043--1071}).
%	%Type = Article
%	\bibitem[{Platt et~al.(1999)}]{platt1999probabilistic}
%	\bibinfo{author}{Platt, J.} et~al. (\bibinfo{year}{1999}).
%	\newblock \bibinfo{title}{Probabilistic outputs for support vector machines and
%		comparisons to regularized likelihood methods}.
%	\newblock {\it \bibinfo{journal}{Advances in large margin classifiers}\/},
%	{\it \bibinfo{volume}{10}\/}, \bibinfo{pages}{61--74}.
%	%Type = Book
%	\bibitem[{Rasmussen \& Williams(2005)}]{rasmussen2005gp}
%	\bibinfo{author}{Rasmussen, C.~E.}, \& \bibinfo{author}{Williams, C. K.~I.}
%	(\bibinfo{year}{2005}).
%	\newblock {\it \bibinfo{title}{Gaussian Processes for Machine Learning}\/}.
%	\newblock \bibinfo{publisher}{The MIT Press}.
%	%Type = Inproceedings
%	\bibitem[{Tang et~al.(2007)Tang, Xiong, Zhong \& Wu}]{tang2007enhancing}
%	\bibinfo{author}{Tang, W.}, \bibinfo{author}{Xiong, H.},
%	\bibinfo{author}{Zhong, S.}, \& \bibinfo{author}{Wu, J.}
%	(\bibinfo{year}{2007}).
%	\newblock \bibinfo{title}{Enhancing semi-supervised clustering: a feature
%		projection perspective}.
%	\newblock In {\it \bibinfo{booktitle}{Proceedings of the 13th ACM SIGKDD
%			international conference on Knowledge discovery and data mining}\/} (pp.
%	\bibinfo{pages}{707--716}).
%	\newblock \bibinfo{organization}{ACM}.
%	%Type = Article
%	\bibitem[{Wang et~al.(2014)Wang, Li, Li \& Yang}]{wang2014constraint}
%	\bibinfo{author}{Wang, H.}, \bibinfo{author}{Li, T.}, \bibinfo{author}{Li, T.},
%	\& \bibinfo{author}{Yang, Y.} (\bibinfo{year}{2014}).
%	\newblock \bibinfo{title}{Constraint neighborhood projections for
%		semi-supervised clustering}.
%	\newblock {\it \bibinfo{journal}{Cybernetics, IEEE Transactions on}\/},  {\it
%		\bibinfo{volume}{44}\/}, \bibinfo{pages}{636--643}.
%	%Type = Article
%	\bibitem[{Wang \& Pan(2014)}]{wang2014semi}
%	\bibinfo{author}{Wang, Y.}, \& \bibinfo{author}{Pan, Y.}
%	(\bibinfo{year}{2014}).
%	\newblock \bibinfo{title}{Semi-supervised consensus clustering for gene
%		expression data analysis}.
%	\newblock {\it \bibinfo{journal}{BioData mining}\/},  {\it
%		\bibinfo{volume}{7}\/}, \bibinfo{pages}{1--13}.
%	%Type = Inproceedings
%	\bibitem[{Weinberger et~al.(2005)Weinberger, Blitzer \&
%		Saul}]{weinberger2005distance}
%	\bibinfo{author}{Weinberger, K.~Q.}, \bibinfo{author}{Blitzer, J.}, \&
%	\bibinfo{author}{Saul, L.~K.} (\bibinfo{year}{2005}).
%	\newblock \bibinfo{title}{Distance metric learning for large margin nearest
%		neighbor classification}.
%	\newblock In {\it \bibinfo{booktitle}{Advances in neural information processing
%			systems}\/} (pp. \bibinfo{pages}{1473--1480}).
%	%Type = Inproceedings
%	\bibitem[{Xing et~al.(2002)Xing, Jordan, Russell \& Ng}]{xing2002distance}
%	\bibinfo{author}{Xing, E.~P.}, \bibinfo{author}{Jordan, M.~I.},
%	\bibinfo{author}{Russell, S.}, \& \bibinfo{author}{Ng, A.~Y.}
%	(\bibinfo{year}{2002}).
%	\newblock \bibinfo{title}{Distance metric learning with application to
%		clustering with side-information}.
%	\newblock In {\it \bibinfo{booktitle}{Advances in neural information processing
%			systems}\/} (pp. \bibinfo{pages}{505--512}).
%	%Type = Inproceedings
%	\bibitem[{Xing et~al.(2012)Xing, Jordan, Russell \& Ng}]{bellet2012similarity}
%	\bibinfo{author}{Xing, E.~P.}, \bibinfo{author}{Jordan, M.~I.},
%	\bibinfo{author}{Russell, S.}, \& \bibinfo{author}{Ng, A.~Y.}
%	(\bibinfo{year}{2012}).
%	\newblock \bibinfo{title}{Similarity learning for provably accurate sparse
%		linear classiï¬cation}.
%	\newblock In {\it \bibinfo{booktitle}{Proceedings of the 29th International
%			Conference on Machine Learning (ICML)}\/} (pp. \bibinfo{pages}{1871--1878}).
%	%Type = Article
%	\bibitem[{Zeng \& Cheung(2012)}]{zeng2012semi}
%	\bibinfo{author}{Zeng, H.}, \& \bibinfo{author}{Cheung, Y.-m.}
%	(\bibinfo{year}{2012}).
%	\newblock \bibinfo{title}{Semi-supervised maximum margin clustering with
%		pairwise constraints}.
%	\newblock {\it \bibinfo{journal}{Knowledge and Data Engineering, IEEE
%			Transactions on}\/},  {\it \bibinfo{volume}{24}\/},
%	\bibinfo{pages}{926--939}.
%	%Type = Article
%	\bibitem[{Zhou et~al.(2004)Zhou, Bousquet, Lal, Weston \&
%		Sch{\"o}lkopf}]{zhou2004learning}
%	\bibinfo{author}{Zhou, D.}, \bibinfo{author}{Bousquet, O.},
%	\bibinfo{author}{Lal, T.~N.}, \bibinfo{author}{Weston, J.}, \&
%	\bibinfo{author}{Sch{\"o}lkopf, B.} (\bibinfo{year}{2004}).
%	\newblock \bibinfo{title}{Learning with local and global consistency}.
%	\newblock {\it \bibinfo{journal}{Advances in neural information processing
%			systems}\/},  {\it \bibinfo{volume}{16}\/}, \bibinfo{pages}{321--328}.
%	
%\end{thebibliography}

%
%\bibliography{LPSP}
%
%\end{document}
